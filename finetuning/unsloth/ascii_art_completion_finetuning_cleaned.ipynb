{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/prasanth-ntu/pookie-llm-finetuning-resources/blob/main/finetuning/unsloth/ascii_art_completion_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FXyY0QcUYXnt"
   },
   "source": [
    "## Completion finetuning using unsloth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKZUSV3pLdEB"
   },
   "source": [
    "For my notes, refer https://prasanth.io/Talks/How-to-Fine-tune-LLMs-with-Unsloth---Complete-Guide-by-Pookie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ekuoswD_iVhr"
   },
   "source": [
    "**Google Colab T4 resources**\n",
    "- System RAM: 12.7 GB\n",
    "- GPU RAM: 15.0 GB\n",
    "- Disk: 112.6 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f0yAF-Tln6uA"
   },
   "outputs": [],
   "source": [
    "# !top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qC3gq2aZdv-l",
    "outputId": "45083505-3910-40dc-83aa-76f0bdcf8775"
   },
   "outputs": [],
   "source": [
    "# Sneak peak at hardware specs\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "try:\n",
    "    # Print System RAM\n",
    "    print(\"System RAM:\")\n",
    "    ram_output = subprocess.check_output([\"free\", \"-h\"])\n",
    "    print(ram_output.decode(\"utf-8\"))\n",
    "\n",
    "    # Print GPU RAM\n",
    "    print(\"\\nGPU RAM:\")\n",
    "    gpu_output_raw = subprocess.check_output(\"nvidia-smi | grep 'MiB' | awk '{print $9, $11}'\", shell=True).decode(\"utf-8\").strip()\n",
    "    gpu_parts = gpu_output_raw.split()\n",
    "    if len(gpu_parts) == 2:\n",
    "        used_gpu = gpu_parts[0]\n",
    "        total_gpu = gpu_parts[1]\n",
    "        print(f\"{used_gpu} (used) / {total_gpu} (total)\")\n",
    "    else:\n",
    "        print(gpu_output_raw)\n",
    "\n",
    "\n",
    "    # Print Disk Space\n",
    "    print(\"\\nDisk:\")\n",
    "    disk_output = subprocess.check_output(\"df -h / | awk 'NR==2{print $2, $3, $4, $5}'\", shell=True)\n",
    "    print(disk_output.decode(\"utf-8\"))\n",
    "\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error executing command: {e}\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Command not found: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XH7UCz77hZVw",
    "outputId": "71654aa4-d3eb-4549-9e31-2693fe88750d"
   },
   "outputs": [],
   "source": [
    "# GPU usage\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EiynQnwhMM2"
   },
   "source": [
    "This notebook makes use of unsloth to finetune a model for a completion task.\n",
    "In this example we will finetune the llama 3.2 base model to generate ascii art. I would recommend using the unsloth library compared to just using the huggingface library as it requires less memory and is faster.\n",
    "\n",
    "Adapted from unsloth notebooks, if something is broken check on:\n",
    "https://unsloth.ai/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "amKhGnqYie7T",
    "outputId": "a7172b20-1aac-4f77-9175-cd3df106a903"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFDobMkSXqxw"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3  peft trl triton\n",
    "!pip install --no-deps cut_cross_entropy unsloth_zoo\n",
    "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "!pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JS6hVGbfn170",
    "outputId": "6263613c-d44b-40a4-8a8a-f5b2ea7a29bc"
   },
   "outputs": [],
   "source": [
    "!pip install -U datesets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZN3IUgafYfEu"
   },
   "source": [
    "### Load base model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NU740tY3j2kt"
   },
   "source": [
    "- Model: [meta-llama/Llama-3.2-3B](https://huggingface.co/meta-llama/Llama-3.2-3B)\n",
    "  - Parameters: 3B\n",
    "  - Variant: pretrained\n",
    "  - modality: text only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KKfnUNmFz7ws"
   },
   "outputs": [],
   "source": [
    "# Takes ~2 mins\n",
    "from unsloth import FastLanguageModel # Makes finetuning LLM faster and efficienrt\n",
    "import torch\n",
    "from google.colab import userdata\n",
    "\n",
    "# Load the pretrained model and the tokenizer from HF model repo\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"meta-llama/Llama-3.2-3B\",\n",
    "    # Sequence longer than this will be truncated. Sequence include both the input tokens (prompts)\n",
    "    # and output tokens (LLM generated output)\n",
    "    max_seq_length = 2048,\n",
    "    # Unsloth intelligently checks your hardware to determine and optimize which lower-precision\n",
    "    # data types are supported (float16 or bfloat16) that provide the best performance & memory efficiency\n",
    "    dtype = None,\n",
    "    # If enabled, model weights are loaded using 4-bit quantization to significanlty reduce memory\n",
    "    # usage by trading off precision\n",
    "    load_in_4bit = False,\n",
    "    token=userdata.get('HF_ACCESS_TOKEN')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-eg28bp9CKq"
   },
   "outputs": [],
   "source": [
    "# Disables the automatic removal of potentially extra spaces around punctuation during the\n",
    "# tokenizer's decoding process, ensuring that whitespace is preserved exactly as tokenized.\n",
    "# In some specific use cases, particularly when dealing with text where the exact spacing is\n",
    "# important (like in the ASCII art example you have, or in code where whitespace matters),\n",
    "# you want to preserve all whitespace precisely as the model generates it.\n",
    "tokenizer.clean_up_tokenization_spaces = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQFFvzlspAb6"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "gzp9-ipOkWDV",
    "outputId": "78c55625-076e-4604-ba29-ce60a4826e09"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uRARyu6GYiK1"
   },
   "source": [
    "### Add lora to base model and patch with Unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "LYobA8h40U9E",
    "outputId": "71b25ed0-bb62-4f10-ce8d-9492db128b89"
   },
   "outputs": [],
   "source": [
    "# More info about parameters: https://huggingface.co/docs/peft/v0.11.0/en/package_reference/lora#peft.LoraConfig\n",
    "# LoRA works by injecting small, trainable low-rank matrices into specific layers of the pre-trained model\n",
    "target_modules = [\n",
    "    # query, key, and value projection layers in the self-attention mechanism\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\",\n",
    "    # output projection layer in the self-attention mechanism\n",
    "    \"o_proj\",\n",
    "    # layers within the feed-forward network (MLP) block\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ]\n",
    "\n",
    "# When adding special tokens\n",
    "train_embeddings = False\n",
    "\n",
    "if train_embeddings:\n",
    "  target_modules = target_modules + [\"lm_head\"]\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model, # base language model\n",
    "    r = 16, # rank of lora matrices according to paper not much loss when set relatively low\n",
    "    target_modules = target_modules,  # On which modules of the llm the lora weights are used (or adapters are inserted)\n",
    "    lora_alpha = 16, # scales the weights of the adapters (more influence on base model), 16 was recommended on reddit\n",
    "    lora_dropout = 0, # Default on 0.05 in tutorial but unsloth says 0 is better\n",
    "    bias = \"none\",    # \"none\" is optimized\n",
    "    use_gradient_checkpointing = \"unsloth\", #\"unsloth\" for very long context, decreases vram\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # scales lora_alpha with 1/sqrt(r), huggingface says this works better\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148,
     "referenced_widgets": [
      "3385191163344d43842fcdefb7806f1c",
      "e92d503f3ce1467989506e33b6dda8a9",
      "aa1c7eb9961948159dd32ceda5a21c1d",
      "a0bd0895e0254f3db80d0965cba20183",
      "fcb76f0f0cc0467f88b602e6cb904925",
      "74101c0dc75345bbaeb3c6f451360075",
      "a9db528bda0d474899c5699e975f9c48",
      "94a08be054124d93994abe79e743010e",
      "59569e47b02d47148580f9497b1f5d7e",
      "c2a72e61fa254a018fddc551132dd702",
      "7f316394be2f46beada14f35bf72494d",
      "3cb5b862385f4b11bdab1a5190cec58f",
      "1f2576280098480c960a7919349d3f26",
      "b3e281ed21ad45a9922b22730954d91a",
      "597b8bb0ba6a42da8aa58088bc8b054a",
      "d4532d7e10474d308ebbab8ee1251b3d",
      "c21267132ad6444e9c3e39f7f0a2335b",
      "277623a0714e4cbd879d897848fb8e55",
      "00e395f344524beabbd2d67724f1181a",
      "84ef498b860741edae3960c1fc9311b6",
      "5195277521a74bf881fbfba4d018ee0d",
      "40c1a0d2c2fd4683ae756434ca6d1618",
      "fde3927cf25a4071ba6516578e5754a2",
      "b59403a98b7f47eebb5411bf6abc7c47",
      "8a6ba1a3df814c3281ed8732dee3d69d",
      "6954d93b31cc470694e3bfc10e0b2f71",
      "172348519c2942ada247e8ce0f07a13a",
      "d227298842394d9b982f538ede554b87",
      "1a0208b25ff4445bb24a7bfe5a9571fb",
      "ea17af8b50974423b59e8ede7a5a1bdc",
      "bd87205e61e740da86f8f342797de3b9",
      "75ef0cbc189841999876da227be4b694",
      "d62069821f894edd9a9ad28e186a2021"
     ]
    },
    "id": "NYsJICcT07cN",
    "outputId": "13e4c030-9974-465f-8f9e-68904e0d90c0"
   },
   "outputs": [],
   "source": [
    "empty_prompt = \"\"\"\n",
    "{ascii_art}\n",
    "\"\"\"\n",
    "\n",
    "# A special marker that signals the end of a sequence to the LLM.\n",
    "# It's crucial for the model to learn where the generated output should end.\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def formatting_prompts_func_no_prompt(examples):\n",
    "  \"\"\"\n",
    "  Formats a batch of ASCII art examples into training prompts for a language model.\n",
    "\n",
    "  Each ASCII art sample is wrapped in a simple template and appended with an\n",
    "  end-of-sequence token.\n",
    "\n",
    "  Args:\n",
    "      examples (dict): A dictionary containing a batch of data, expected to have\n",
    "                       an \"ascii\" key with a list of ASCII art strings.\n",
    "\n",
    "  Returns:\n",
    "      dict: A dictionary with a single key \"text\" containing a list of formatted\n",
    "            training prompt strings.\n",
    "  \"\"\"\n",
    "  print(f\"len(examples): {len(examples)}\")\n",
    "  ascii_art_samples = examples[\"ascii\"]\n",
    "  print(f\"len(ascii_art_samples): {len(ascii_art_samples)}\")\n",
    "  training_prompts = []\n",
    "  for ascii_art in ascii_art_samples:\n",
    "      training_prompt = empty_prompt.format(ascii_art=ascii_art) + EOS_TOKEN\n",
    "      training_prompts.append(training_prompt)\n",
    "  return { \"text\" : training_prompts, }\n",
    "\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset_org = load_dataset(\"pookie3000/ascii-cats\", split = \"train\", download_mode=\"force_redownload\")\n",
    "dataset = dataset_org.map(formatting_prompts_func_no_prompt, batched = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghijf8GxNORq"
   },
   "source": [
    " ### Visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "S1_7YFoUzw5F",
    "outputId": "db7377d7-e4f2-492b-ab78-d034ea7501a3"
   },
   "outputs": [],
   "source": [
    "EOS_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 156
    },
    "id": "o_ByxkXAzO6p",
    "outputId": "f2c40ac6-4a0a-4238-a717-885077490aa3"
   },
   "outputs": [],
   "source": [
    "print(f\"dataset_org: {dataset_org}\")\n",
    "print(f\"dataset: {dataset}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "D8Qwl_t015mi",
    "outputId": "97320feb-28c5-496e-e2c3-51adb9960863"
   },
   "outputs": [],
   "source": [
    "# Compare the \"ascii\" and \"text\" in raw format\n",
    "dataset[0][\"ascii\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "WWpJMrcq18ZV",
    "outputId": "42adba83-674e-4fe4-88e7-8baf7045916e"
   },
   "outputs": [],
   "source": [
    "dataset[0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 833
    },
    "id": "Eo003oh50PXS",
    "outputId": "9ae968b1-8a7a-43b2-b92b-7198dad5e6d4"
   },
   "outputs": [],
   "source": [
    "# Compare the \"dataset_org\" and \"dataset\" for a random sample\n",
    "import random\n",
    "random_index = random.randint(0, len(dataset) - 1)\n",
    "print(f\"Random index: {random_index}\")\n",
    "\n",
    "print(f\"\\n=== dataset_org ===\")\n",
    "# print(f\"Content at random index:\\n{dataset_org[random_index]}\")\n",
    "for k,v in dataset_org[random_index].items():\n",
    "  print (f\"--- {k} ---\\n{v}\")\n",
    "\n",
    "print(f\"\\n=== dataset ===\")\n",
    "# print(f\"Content at random index:\\n{dataset_org[random_index]}\")\n",
    "for k,v in dataset[random_index].items():\n",
    "  print (f\"--- {k} ---\\n{v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 590
    },
    "id": "yeQ6cV5BMn6-",
    "outputId": "477413ba-21b3-46bc-bb4c-88607407d062"
   },
   "outputs": [],
   "source": [
    "for i, sample in enumerate(dataset):\n",
    "    print(f\"\\n------ Sample {i + 1} ----\")\n",
    "    print(sample[\"text\"])\n",
    "    if i > 2:\n",
    "      break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "8583d4728ff84513b2db79243be3e949",
      "6de3f183fb0b45ed8638ae8b7a050f7e",
      "f990b4d4506041ffba69f04ce876d46e",
      "4e56b2423ae94c2ab24e5b64e152bb13",
      "5e67bed2ad6147b7abb1e8811fbfac48",
      "cbfce7f17bd8451e9abe66b0e921a82a",
      "46e3bda6efc84b90bb17fe66bb53451a",
      "1d54a4d48f274ce98b4da6bd4233d23d",
      "8a75c9857a0d44e2a25afb9200b4555e",
      "c745ad91b17d45029580e99a0953c203",
      "ca081f479d3e44ceb51956dd623abccb"
     ]
    },
    "id": "4PNMTxya08bi",
    "outputId": "ea4e4568-7d30-4eac-abf8-0d23d6e567ff"
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer # trl - library designed for finetuning LLMs on a given dataset\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "# Configure the training process by specifying the model, tokenizer, dataset, anda all necessary\n",
    "# hyperparameters for the SFTTrainer to begin finetuning the LoRA-adapted model.\n",
    "trainer = SFTTrainer( # Finetuning LLMs on a supervised dataset\n",
    "    model = model, # LLM to be finetuned (the LoRA-adapted model prepared earlier)\n",
    "    tokenizer = tokenizer, # Tokenizer associated with the model\n",
    "    train_dataset = dataset, # Formatted dataset we prepared earlier for training\n",
    "    dataset_text_field = \"text\", # Col in the dataset that contains the text data for training\n",
    "    max_seq_length = 2048, # Should match with setting when loading the model\n",
    "    dataset_num_proc = 2, # Number of processes to use for processing the dataset\n",
    "    # Define all the hyperparameters and configuration settings for the training process\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2, # batch size per GPU (or CPU if not using a GPU) during training\n",
    "        # The gradients are accumulated over this many batches before a single optimization step (parameter update) is performed\n",
    "        # (2 batches/GPU * 4 batches = 8 in this case). This is useful when you don't have enough GPU memory to fit a large batch directly\n",
    "        gradient_accumulation_steps = 4, # process 4 batches before updating parameters (parameter update == step)\n",
    "        # full passes through the training dataset\n",
    "        num_train_epochs = 5, # between 1 - 3 to prevent overfitting\n",
    "        learning_rate = 2e-4, # Initial learning rate for the optimizer\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1, # Log training information (like loss) ever step\n",
    "        optim = \"adamw_8bit\", # 8-bit AdamW optimizer, which is memory-efficient. Often used with techniques like LoRA\n",
    "        weight_decay = 0.01, # A regularization parameter to prevent overfitting\n",
    "        # A \"linear\" scheduler decreases the learning rate linearly from the initial value to 0 over the course of training\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\", # The directory where training outputs (like checkpoints and logs) will be saved.\n",
    "        # Specifies where to report training progress (e.g., \"tensorboard\", \"wandb\", \"none\").\n",
    "        report_to = \"none\".# Setting it to \"none\" disables reporting to external services.\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JzHaVYIP1AKp",
    "outputId": "e2c9cc56-9e0a-4c17-dded-920bd73b3dd1"
   },
   "outputs": [],
   "source": [
    "# Takes ~4 mins if num_train_epochs = 5\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "0rWLawD54AbS",
    "outputId": "4ee13922-6dfa-4898-8fa2-ca025bbd0aea"
   },
   "outputs": [],
   "source": [
    "# After training the model\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "id": "-7WXqRCP4CxH",
    "outputId": "ab6a88a1-d310-4029-8509-8713e1f72f5e"
   },
   "outputs": [],
   "source": [
    "!ls -a -l outputs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "id": "MmjJkzwH6f5b",
    "outputId": "adeaadcf-f0f2-454b-9d9f-69a5075f0647"
   },
   "outputs": [],
   "source": [
    "!ls -a -l -h outputs/checkpoint-130/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UlSmY8m4grb"
   },
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "-IqA3IbOcTuz",
    "outputId": "2def14fb-97cc-4328-ef29-efdc48553411"
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer # Streams the generated text from the model token by token\n",
    "\n",
    "def generate_ascii_art(model):\n",
    "    \"\"\"\n",
    "    Generates ASCII art using the finetuned language model.\n",
    "\n",
    "    Applies Unsloth inference optimizations, prepares an empty input,\n",
    "    sets up a text streamer for token-by-token output, and generates\n",
    "    ASCII art using the model's generate method.\n",
    "\n",
    "    Args:\n",
    "        model: The finetuned language model object.\n",
    "    \"\"\"\n",
    "    FastLanguageModel.for_inference(model) # Applies Unsloth's optimizations specifically for inference\n",
    "\n",
    "    # Prepares the inputs for the model\n",
    "    inputs = tokenizer(\n",
    "        # Tokenizes an empty string. Since this model is finetuned for completion (generating ASCII\n",
    "        # art without a specific text prompt), an empty string serves as the starting point\n",
    "        \"\",\n",
    "        return_tensors = \"pt\" # Specifies that the output should be PyTorch tensors\n",
    "      ).to(\"cuda\") #  Moves the input tensors to the GPU for faster processing.\n",
    "\n",
    "    text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "    # https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/text_generation#transformers.GenerationMixin\n",
    "    # https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "    for token in model.generate( # A versatile function for controlling text generation\n",
    "        **inputs, # Unpacks the input tensors created earlier\n",
    "        streamer = text_streamer,\n",
    "        max_new_tokens = 100 # Model will generate up to 100 tokens (max number of newe tokens) of ASCII art\n",
    "      ):\n",
    "        print(token)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 920
    },
    "id": "DLxH5W-2cmrH",
    "outputId": "b0415e43-7feb-451a-9e86-452d1465d550"
   },
   "outputs": [],
   "source": [
    "for _ in range(3):\n",
    "  print(\"-\"*40)\n",
    "  generate_ascii_art(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YKQLPkQGP3hR"
   },
   "source": [
    "## Saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRiNf0EthMM4"
   },
   "source": [
    "### Save lora adapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yh5vi-YshMM4"
   },
   "source": [
    "This is both useful for inference and if you want to load the model again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TyU_mAeS_R9P"
   },
   "source": [
    "The code below only saves the **small LoRA adapter weights** and does not save the full base model. When you load this model later for inference or further finetuning, you would typically load the original base model (`meta-llama/Llama-3.2-3B`) and then load these LoRA weights on top of it. This is beneficial because the LoRA adapter files are much smaller than the full model, making them easier to store and share. This format is useful if you want to continue finetuning or use the model within frameworks that support loading LoRA adapters separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130,
     "referenced_widgets": [
      "3ebfd8783ea7424a842ae57cf9d71175",
      "578a2080df8b424484b28dd4766c0246",
      "72fd43dc85484bff9662b70ba3941501",
      "761abd7aaadf4fbb8d495e5b9eb1cecd",
      "334dfb34f93d4c4a81ebd3d7fc2226c9",
      "10ead4e196c44cc59d1100edefd3299a",
      "60b32ad5fda745318149f04eba686f19",
      "a574e7ae64bc43e68a2867ec3253f0db",
      "be36dea50ba8494688c30ac3487aa1b5",
      "6fccc5e254a3470e894ff9b30007dae4",
      "8fb92a2166a040478572b5257488be6d",
      "fda843cef4b5421687d5568ae36d8cc4",
      "579a883826134df288106dfbd8b6d5df",
      "524ae273d9534596a6d6c0d8dc7eb56e",
      "9dc18eb9c67f4d56b08a0058023d7a19",
      "4a89eee68efa4314a5ef958794010fa2",
      "ae5abff18fa34a96b7c3b6042ac3bbce",
      "2f67a7c51df44644bb1f32baa9f72de7",
      "713f64694ce648e0bcaf8eeb3eb4f444",
      "323dceaaccf049ee878ce0f0706bbaeb",
      "5c2ed7bf2add402e998f09e3c8cb5864",
      "4b05e7a6d3cd460197790fff20c29bf1",
      "a5f3ce04099344db80ddfc80f926fe05",
      "07aa7e6efd1b4629922d2298602dc507",
      "62e93e7f15b84040bf97a8b9b20f9f83",
      "d529b28234864fee899c0f6a18fb7027",
      "57ccadb81e2746cfa116f836c751bb07",
      "db6e8fb935a94128afcb56dc11eaa1f8",
      "24591c5802084bc38d111043eb5a5d1e",
      "f1e16f5869554b379aa52e78799bbe90",
      "a2996970070d4c9c92c8eebcdc6f4ed0",
      "1fcd7064cb544587926c9cb23073cbbc",
      "0044b1221f0647c5b17671f2759153d9"
     ]
    },
    "id": "L0eawVm7hMM4",
    "outputId": "d0f46f14-cdb8-4ba5-86f9-b8954ee56d97"
   },
   "outputs": [],
   "source": [
    "# Takes only few secs\n",
    "model.push_to_hub(\n",
    "    # \"pookie3000/Llama-3.2-3B-ascii-cats-lora\",\n",
    "    \"prasanthntu/Llama-3.2-3B-ascii-cats-lora\",\n",
    "    tokenizer,\n",
    "    token = userdata.get('HF_ACCESS_TOKEN')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "msSaJLcphMM4"
   },
   "source": [
    "### Merge model with lora weights and save to gguf\n",
    "\n",
    "You can then do inference locally with Ollama or llama.cpp\n",
    "\n",
    "##### Popular quantization methods\n",
    "\n",
    "- **q4_k_m**  \n",
    "  4bit quantization. Low memory. All models you pull with ollama uses this quantization.\n",
    "- **q8_0**  \n",
    "  8bit quantization. Medium memory.\n",
    "- **f16**  \n",
    "  16 bit quantization. A lot of models are already in 16 bit so then no quantization happens\n",
    "- **not_quantized**  \n",
    "  Often same as f16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWaJ1W7j_zqv"
   },
   "source": [
    "This code snippet does two main things before saving:\n",
    "\n",
    "- It **merges the LoRA adapter weights with the base model weights**. This creates a single, consolidated model where the finetuning changes are incorporated directly into the base model's parameters.\n",
    "- It then **saves this merged model in the GGUF format**. GGUF is a binary format designed for efficient loading and inference of large language models on various hardware, particularly CPUs and consumer GPUs, using tools like llama.cpp and Ollama. The `quantization_method=\"q4_k_m\"` argument specifies a quantization method (4-bit in this case) to further reduce the model size and improve inference speed, often with minimal loss in quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "b271bab0ab8547a99dc21192c47e4814",
      "c2cc73dc8e34404fb65961d2824fc632",
      "b513acb090504fcd9e656d67bd1e159b",
      "8663e03fe09e42ef9a5019c2e247b09d",
      "36c8f8cb683d499eab7dc42b51bb46a2",
      "5f51abc622444da492c0c975388f363e",
      "c24d4b68516f469dafb8d45163b10fff",
      "465ce0885191487689bd1b06af12cb38",
      "22b7e48d64d6481db1c066e591e5825e",
      "dfae15aec75a42cab98d8b979f532c67",
      "b36a31c1ec9d4812ae68136f4e6d3e11",
      "bb03239890da46efbc8fecebf8dfeb56",
      "ed7dd5278f1a4dc89b8d971d4e480610",
      "ec066155776d49dc8384c440b4de5cf2",
      "afe655edb44346e8837dec02f27f5e87",
      "ad2d5cb32e924234a9b8732f3183ee66",
      "9a4ddf2f72e4477b97f99e100032b658",
      "793414a578bb4ab3b3117580e2366ad2",
      "be5a52293aa54672bc26efdd09509ec0",
      "a6cdb4a72c8f4573991447e46082b3a1",
      "688d8eaf3bc848bf82862a48d238328e",
      "a27b26116d9c4ab39984bd9a29b518e8"
     ]
    },
    "id": "WwSp6tH51FoG",
    "outputId": "ed66f63e-b28f-4c2f-f6b4-fa92c46b8209"
   },
   "outputs": [],
   "source": [
    "# Took around ~X(=15) mins\n",
    "model.push_to_hub_gguf(\n",
    "    # \"pookie3000/Llama-3.2-3B-ascii-cats-lora-q4_k_m-GGUF\",\n",
    "    \"prasanthntu/Llama-3.2-3B-ascii-cats-lora-q4_k_m-GGUF\",\n",
    "    tokenizer,\n",
    "    quantization_method=\"q4_k_m\",\n",
    "    token = userdata.get('HF_ACCESS_TOKEN')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oKde29amhMM4"
   },
   "source": [
    "### Load model and saved lora adapters\n",
    "For if you want to continue finetuning or want to do inference using the model in safetensor format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ph2aR1jNJO-a",
    "outputId": "30c7501f-7866-43f8-9cc2-8d25d9b53dd3"
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel # Makes finetuning LLM faster and efficienrt\n",
    "import torch\n",
    "from google.colab import userdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427,
     "referenced_widgets": [
      "f2c8c55084594a35b14c89f18bbbfde1",
      "f4f4989f65fa4a2e807c4ea821f1f126",
      "d10c3184d771443592c20c06282aca3b",
      "4ada55209de847e8a9ecc02757efdd89",
      "103eeb398702426bbb6e95d91d584155",
      "d990c19ebd0443a58020aa8d94cf2149",
      "0f8dc33a16364ba2bf8d25a83f127f04",
      "824ac47004f449dfb448b62e1b7f3a57",
      "d2caa1fef67a4b06ae7013d231813830",
      "12631a82ae0f4956b6637f14fa8a823e",
      "70fa4ba9fa4a488db862329e957124d0",
      "4bfc035d235847ec8ecd1cdb21856a63",
      "3e85c8aa63cd46f1bae6ded397fffd8a",
      "20db76fdb0234ccbbb638b6901057abb",
      "21e4ca8940bc4ecd9a48283681371cd1",
      "09f2b90ae5b645e4a5f6f12270c37e47",
      "7db006fa8511466aa93b30c9e2d6caa6",
      "349fed8485394749857b0de20c24391c",
      "31aacd1c00c54d7e97d9e7f173ef3db7",
      "968454401d754f5d95f5a3b7d12b4b03",
      "1748a7094e794ddca71bee7a9e64b3b4",
      "570764d048f94ca9991eb315ef091ef4",
      "76d3f1a79e0a44058a4c1d6b65bef286",
      "19d15a9a41274c65be682436e3784cc9",
      "e669dc07bb9c45f491efd30c0b9eebca",
      "8a62c98892844fe79e55c6a4989e853b",
      "8bbb34b317044df9b70adadb3ce06036",
      "0d12bb1b982d42199f93ca264d0a276f",
      "ef7ff8ebe14d41218c9c33c4d5790fef",
      "13bcd7d87dd44752bacc9c6a393a24a1",
      "01647363ce3c4e50a4268fb4783b3c95",
      "6e2092acff41429b956c8abd55f386c7",
      "8b69368c5a2e49c5a89f43fd53a750a7",
      "01cf90b9c6ee47ec9d5785910c4edcea",
      "f76a2a559fdb4f3aa13e0e7c654daae6",
      "94d07dc131a44a15a7fb4369c7bf539f",
      "6dce859f76aa4b2182f40f04f4faaa0e",
      "e1b52378396a495da3217fe3f420a7ad",
      "e31e96ac2d4f496baab82085739417c9",
      "3a07fd3d1c4d494e92c7bf93a8a61559",
      "ef0211774d6a48b38388a004371c24ee",
      "f31f0bf551be4833ba3327e1df42eb29",
      "e4f74ac0a5114ffd8c9da854a6c8ede3",
      "7722cfd8830840938bba6a2a5600da92",
      "16dd60b56d0c4a0e8f35c7ea58d173d9",
      "2762f7a8e26c454c8b649ad7b9d31e29",
      "ed2f007bb211491b83acd78e071b4ca9",
      "8a7c1a7b257c43f68b1ec82775b27010",
      "c20d60dbd28c41c98ae8cd1b41cbdead",
      "2e12ceb9cabe49e890199f4b3e7415f0",
      "9418e559907c4039b5cb7b41bd412708",
      "2c03b3f109a74ce3ab060f4e60459e5e",
      "c7e6b1e6f58b4d7cbc789117536c1f11",
      "566e65f280c741b194a866e786cd975c",
      "a9c27865bf8b458fb63ab861cd9a3ff6",
      "20a2a0831b9b4c82ae91336f336dee27",
      "6d2146e7dc854e4982779408e295dc14",
      "de594b15ce22468595e5ba187e682250",
      "7dd97c62e21a403390a71a446c93c99c",
      "8b00cef503db47658041e654cd3d8194",
      "2486fc4c4bfc408eb2a72f954aca3196",
      "505310446ec54654b84f4c578af68cab",
      "081389aedbee43f2aaac3fc6317669e8",
      "52578141b60040708d31013f9ce62a4c",
      "2f7e68526dd443e88144b08928930fbe",
      "645f3a39fb6b42bbb732e73a046d453f",
      "bb2280e079bc40fe8ecc0d20b19ec0d0",
      "92348f4098c7455eb3f98f5b451e5835",
      "07fb466d28eb40fdb281061c79e7d488",
      "8e83d4a99e714387a35147e30915cb5e",
      "78b2f6a5045f4832a86d35899263befe",
      "424e48f72e5b450d91562575471cc3ef",
      "d00856de764a44f0b30ed59e0a104af2",
      "d0667092f585421bacfcf022a9d009a3",
      "135df1dfba624afa9abecf2deb7b7528",
      "d152aa2e887145e3bcee3e41e5162ce3",
      "6ddb49a5658e44d5942c81efed8f1557",
      "6ad163ef6991432aa62f2c89ee0608e6",
      "3fb9d6084b3e4241aae1d1d3a04d0ed0",
      "7f87964e44f8499eb3a15e1045311a50",
      "b6aac856ef45473094c661c83b6cee18",
      "4fe39f8c9a9d426284c7354c64845415",
      "ce9fddafb8a64f1fb96d15c2d925027a",
      "33017c9d314047659998f7c4673a266d",
      "cf1fd2d8997342f095ea574f6d0bcc59",
      "f73090ce669848b99af6cbfea7c51eec",
      "d238cf934cf143fe93c80f71357e6697",
      "95e0ac902efd44b69ddbaab75ef06894",
      "9c03cecbcfe44a4ba24e2a0f6226600b",
      "12748474d0c04e77900bc6e411ac5c12",
      "75d563c5f21c4501a0699ebb32f8d1f7",
      "be409b4959234fb0853480491ea9590e",
      "964a166864ba44458678aa80cbb81911",
      "69f33e553d404229a4935c7ec3234005",
      "a9e45598303745f6b55c8a9876a532f2",
      "4242e1e672734b5aaeede2aafb9a235b",
      "92ed30f0c30846ef84dee6d81d7e329a",
      "23731e54d11e46d4a3083b7cfedfe5a2",
      "20b44dbba582451c93676c6045fe798f"
     ]
    },
    "id": "IJ-o2dywhMM4",
    "outputId": "b2f8c0da-46cf-4f58-863a-d38062de8a77"
   },
   "outputs": [],
   "source": [
    "# Takes ~5 mins\n",
    "# Note: If the code fails restart and do the pip installations, and run this code directly\n",
    "from transformers import TextStreamer\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    # model_name=\"pookie3000/Llama-3.2-3B-ascii-cats-lora\",\n",
    "    model_name=\"prasanthntu/Llama-3.2-3B-ascii-cats-lora\",\n",
    "    max_seq_length = 2048,\n",
    "    dtype = None,\n",
    "    load_in_4bit = False,\n",
    "    token=userdata.get('HF_ACCESS_TOKEN')\n",
    ")\n",
    "\n",
    "\n",
    "def generate_ascii_art(model):\n",
    "    FastLanguageModel.for_inference(model)\n",
    "    inputs = tokenizer(\"\", return_tensors = \"pt\").to(\"cuda\")\n",
    "    text_streamer = TextStreamer(tokenizer)\n",
    "    # https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/text_generation#transformers.GenerationMixin\n",
    "    # https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "    for token in model.generate(**inputs, streamer = text_streamer, max_new_tokens = 100):\n",
    "        print(token)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7BN0zDaZJrMH",
    "outputId": "54b513a3-660a-40f5-eab3-a5e255ab7229"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ypyBA709JsGd",
    "outputId": "534372f1-0447-4fdc-bcf6-2791551a12f7"
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "  print(\"-\"*30)\n",
    "  generate_ascii_art(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1f44701a"
   },
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TuyBo2Nc2wMd"
   },
   "source": [
    "## Why set `max_seql_length = 2048` while the actual context length of Llama 3.2 (text only) modek is 128K?\n",
    "While the Llama 3.2 model can handle a context length of up to 128K tokens, there are several reasons why you might set `max_seq_length` to a smaller value like 2048 in a finetuning notebook, especially when using libraries like Unsloth:\n",
    "\n",
    "- **Computational Resources**: Processing very long sequences requires significantly more computational resources (GPU memory and time). Finetuning with extremely long sequences can be prohibitively expensive or even impossible on consumer-grade hardware or platforms with limited resources like Google Colab (even with a T4 GPU). A `max_seq_length` of 2048 is a common and manageable size for many tasks and hardware setups.\n",
    "- **Memory Constraints**: The memory required for training increases quadratically with the sequence length. Setting a lower `max_seq_length` is a crucial technique for reducing memory consumption and avoiding \"out of memory\" errors during training. Unsloth helps make training more memory efficient, but there are still limits based on the available GPU RAM.\n",
    "- **Task Requirements**: The optimal `max_seq_length` also depends on the specific task you are finetuning the model for. For a task like generating ASCII art based on a short prompt, a context length of 2048 might be more than sufficient to capture the necessary information and generate the desired output. Using a much larger context might not provide any significant benefit for this particular task and would only increase training costs.\n",
    "- **Dataset Characteristics**: The nature of your training data also plays a role. If your training examples typically consist of sequences much shorter than 128K tokens, setting `max_seq_length` to a value that accommodates most of your training data without excessive padding or truncation is a reasonable approach.\n",
    "- **Training Efficiency**: Shorter sequence lengths generally lead to faster training iterations (steps), as less computation is required per token. This can allow for quicker experimentation and faster convergence during the finetuning process.\n",
    "\n",
    "In summary, while the base model has a large potential context window, the `max_seq_length` during finetuning is often set based on a balance of available computational resources, memory constraints, the specific requirements of the finetuning task, and the characteristics of the training data. 2048 is a common and practical choice for many finetuning scenarios on typical hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgQb7C3gxMlS"
   },
   "source": [
    "## `get_peft_model()` function explained\n",
    "\n",
    "*   **`model`**: The base language model object you loaded previously (`meta-llama/Llama-3.2-3B` in this case).\n",
    "*   **`r = 16`**: This is the \"rank\" of the low-rank matrices used in LoRA. A higher rank means more trainable parameters and potentially more expressiveness, but also higher memory usage and slower training. A value of 16 is a common starting point and often provides a good balance.\n",
    "*   **`target_modules = target_modules`**: This specifies the list of module names where the LoRA adapters will be inserted.\n",
    "*   **`lora_alpha = 16`**: This is a scaling factor for the LoRA weights. A higher `lora_alpha` gives more weight to the LoRA adaptations compared to the original pre-trained weights. A value equal to `r` (16 in this case) is a common practice.\n",
    "*   **`lora_dropout = 0`**: This sets the dropout rate for the LoRA layers. Dropout is a regularization technique to prevent overfitting. Setting it to 0 means no dropout is applied to the LoRA layers. Unsloth documentation often suggests 0 for better performance.\n",
    "*   **`bias = \"none\"`**: This specifies whether bias terms should be trained alongside the LoRA weights. Setting it to `\"none\"` means no bias terms are trained, which is often recommended for LoRA.\n",
    "*   **`use_gradient_checkpointing = \"unsloth\"`**: This enables gradient checkpointing, a technique that reduces memory usage during training by recomputing gradients for certain layers instead of storing them. Setting it to `\"unsloth\"` uses Unsloth's optimized implementation of gradient checkpointing, which is particularly useful for long sequences.\n",
    "*   **`random_state = 3407`**: This sets the random seed for reproducibility.\n",
    "*   **`use_rslora = False`**: RSLora is a variation of LoRA that scales `lora_alpha` with `1/sqrt(r)`. Setting it to `False` uses the standard LoRA scaling.\n",
    "*   **`loftq_config = None`**: LoftQ is a method that can be used with LoRA for quantization-aware finetuning. Setting it to `None` means LoftQ is not used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0acb77f4"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
